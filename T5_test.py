from transformers import T5Tokenizer, T5ForConditionalGeneration
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model_path = "./flan_t5_medsplit/checkpoint-final"  # –∏–ª–∏ checkpoint-1000
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)

text = "–¢–æ–≤–∞—Ä–ü–æ—Å—Ç–∞–≤–∫–∏: –ê–∑–∏—Ç—Ä–æ–º–∏—Ü–∏–Ω-–≠–¥–≤–∞–Ω—Å–¥ –∫–∞–ø—Å 500–º–≥ ‚Ññ3 –±–ª–∏—Å—Ç | –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–¢–æ–≤–∞—Ä–∞: –ê–∑–∏—Ç—Ä–æ–º–∏—Ü–∏–Ω-–≠–¥–≤–∞–Ω—Å–¥ –∫–∞–ø—Å—É–ª—ã, 500 –º–≥, 3 —à—Ç. - –±–ª–∏—Å—Ç–µ—Ä (1)  - –ø–∞—á–∫–∞ –∫–∞—Ä—Ç–æ–Ω–Ω–∞—è | –¢–æ—Ä–≥–æ–≤–æ–µ–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: –ê–∑–∏—Ç—Ä–æ–º–∏—Ü–∏–Ω-–≠–¥–≤–∞–Ω—Å–¥ | –î–æ–∑–∏—Ä–æ–≤–∫–∞: 500 –º–≥ | –õ–µ–∫–§–æ—Ä–º–∞: –ö–ê–ü–°–£–õ–´ | –ü–µ—Ä–≤–∏—á–Ω–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ù–∞–∑–≤–∞–Ω–∏–µ: –ë–õ–ò–°–¢–ï–† | –ü–µ—Ä–≤–∏—á–Ω–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: 3.0 | –í—Ç–æ—Ä–∏—á–Ω–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ù–∞–∑–≤–∞–Ω–∏–µ: –ö–ê–†–¢–û–ù–ù–ê–Ø –ü–ê–ß–ö–ê | –í—Ç–æ—Ä–∏—á–Ω–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: 1 | –ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ö–æ–ª–≤–æ: 3.0"

inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(device)
outputs = model.generate(**inputs, max_length=256)
decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

print("üîé decoded:", decoded)
