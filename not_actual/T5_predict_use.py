import pandas as pd
from transformers import T5Tokenizer, T5ForConditionalGeneration
from tqdm import tqdm
import torch
import re
import os
import random

# === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã ===
excel_path = "./data/nlp_dataset_distinct.xlsx"
output_path = "./data/predicted_tokenized_output_random_sample.xlsx"
model_path = "./flan_t5_medsplit/checkpoint-final"
sheet_name = 0

start_row = 100       # –°—Ç–∞—Ä—Ç–æ–≤–∞—è —Å—Ç—Ä–æ–∫–∞ (–≤–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)
end_row = 1000        # –ö–æ–Ω–µ—á–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ (–∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ)
sample_size = 10      # –°–∫–æ–ª—å–∫–æ —Å–ª—É—á–∞–π–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –≤—ã–±—Ä–∞—Ç—å

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ===
assert os.path.isdir(model_path), f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –º–æ–¥–µ–ª–∏: {model_path}"
tokenizer = T5Tokenizer.from_pretrained(model_path)
model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)
model.eval()

# === –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ ===
def build_input(example: dict) -> str:
    fields = [
        "–¢–æ–≤–∞—Ä–ü–æ—Å—Ç–∞–≤–∫–∏", "–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ–¢–æ–≤–∞—Ä–∞", "–¢–æ—Ä–≥–æ–≤–æ–µ–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ",
        "–î–æ–∑–∏—Ä–æ–≤–∫–∞", "–õ–µ–∫–§–æ—Ä–º–∞", "–ü–µ—Ä–≤–∏—á–Ω–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ù–∞–∑–≤–∞–Ω–∏–µ",
        "–ü–µ—Ä–≤–∏—á–Ω–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", "–í—Ç–æ—Ä–∏—á–Ω–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ù–∞–∑–≤–∞–Ω–∏–µ",
        "–í—Ç–æ—Ä–∏—á–Ω–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", "–ü–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∞—è–£–ø–∞–∫–æ–≤–∫–∞–ö–æ–ª–≤–æ"
    ]
    return " | ".join([f"{field}: {example.get(field, '')}" for field in fields])

# === –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∏ —Ä–∞–∑–±–æ—Ä ===
def predict_parts(example: dict) -> dict:
    input_text = build_input(example)
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True, max_length=512).to(device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=256)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("üîé decoded:", decoded)
    result = {}
    for match in re.finditer(r"([\w_]+):\s?([^|]+)", decoded):
        key, value = match.group(1).strip(), match.group(2).strip()
        result[f"{key}_–¢–ü_pred"] = value
    return result

# === –ß—Ç–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Å—Ç—Ä–æ–∫ –∏ —Å–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ ===
print(f"\nüîÑ –ß—Ç–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ {start_row}‚Äì{end_row} –∏–∑ Excel: {excel_path}")
df_range = pd.read_excel(
    excel_path,
    sheet_name=sheet_name,
    skiprows=range(1, start_row + 1),
    nrows=end_row - start_row
).fillna("")

# === –°–ª—É—á–∞–π–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ N —Å—Ç—Ä–æ–∫ ===
assert sample_size <= len(df_range), "‚ùå sample_size –±–æ–ª—å—à–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ!"
df_sample = df_range.sample(n=sample_size, random_state=42).reset_index(drop=True)

# === –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ===
tqdm.pandas(desc="ü§ñ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤")
predictions = df_sample.progress_apply(lambda row: predict_parts(row.to_dict()), axis=1)

# === –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º ===
predicted_df = pd.DataFrame(predictions.tolist())
result_df = pd.concat([df_sample, predicted_df], axis=1)

# === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ===
result_df.to_excel(output_path, index=False)
print(f"\n‚úÖ {sample_size} —Å–ª—É—á–∞–π–Ω—ã—Ö —Å—Ç—Ä–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_path}")
